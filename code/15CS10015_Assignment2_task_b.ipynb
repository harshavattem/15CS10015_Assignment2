{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsha/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import data_loader\n",
    "import module\n",
    "from data_loader import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "\n",
    "ctx = mx.cpu()\n",
    "data_ctx = ctx\n",
    "model_ctx = ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader()\n",
    "\n",
    "class CustomDataset:\n",
    "    \n",
    "    def __init__(self, mode, dataset = 'all'):\n",
    "        self.x, self.y = dl.load_data(mode, dataset)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_data = mx.gluon.data.DataLoader(CustomDataset('train', 'train'), batch_size, shuffle=True)\n",
    "test_data = mx.gluon.data.DataLoader(CustomDataset('train', 'validation'), batch_size, shuffle=False)\n",
    "\n",
    "epochs = 10\n",
    "num_examples = len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        data = mx.ndarray.cast(data, dtype='float32')\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = [1024, 512, 256]\n",
    "lout = 10\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(layer[0], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(layer[1], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(layer[2], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(lout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Uniform(.1), ctx=model_ctx, force_reinit=True)\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_arr = []\n",
    "for e in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        \n",
    "        with autograd.record():\n",
    "            data = mx.ndarray.cast(data, dtype='float32')\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "    \n",
    "    loss_arr.append(cumulative_loss/num_examples)\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "          (e, cumulative_loss/num_examples, train_accuracy, test_accuracy))    \n",
    "\n",
    "loss_vanilla = loss_arr.copy()\n",
    "\n",
    "filename = os.path.join('weights','b_vanilla.params')\n",
    "net.save_parameters(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Normal(sigma=.05), ctx=model_ctx, force_reinit=True)\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_arr = []\n",
    "for e in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        \n",
    "        with autograd.record():\n",
    "            data = mx.ndarray.cast(data, dtype='float32')\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "    \n",
    "    loss_arr.append(cumulative_loss/num_examples)\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "          (e, cumulative_loss/num_examples, train_accuracy, test_accuracy))    \n",
    "\n",
    "loss_norm_init = loss_arr.copy()\n",
    "\n",
    "filename = os.path.join('weights','b_normal.params')\n",
    "net.save_parameters(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xavier Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=model_ctx, force_reinit=True)\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_arr = []\n",
    "for e in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        \n",
    "        with autograd.record():\n",
    "            data = mx.ndarray.cast(data, dtype='float32')\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "    \n",
    "    loss_arr.append(cumulative_loss/num_examples)\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "          (e, cumulative_loss/num_examples, train_accuracy, test_accuracy))    \n",
    "    \n",
    "loss_xavier_init = loss_arr.copy()\n",
    "\n",
    "filename = os.path.join('weights','b_xavier.params')\n",
    "net.save_parameters(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonal Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Orthogonal(scale=1.414, rand_type='uniform'), ctx=model_ctx, force_reinit=True)\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_arr = []\n",
    "for e in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        \n",
    "        with autograd.record():\n",
    "            data = mx.ndarray.cast(data, dtype='float32')\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "    \n",
    "    loss_arr.append(cumulative_loss/num_examples)\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "          (e, cumulative_loss/num_examples, train_accuracy, test_accuracy))   \n",
    "    \n",
    "loss_ortho_init = loss_arr.copy()\n",
    "\n",
    "filename = os.path.join('weights','b_ortho.params')\n",
    "net.save_parameters(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = [1024, 512, 256]\n",
    "lout = 10\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.BatchNorm())\n",
    "    net.add(gluon.nn.Dense(layer[0], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(layer[1], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(layer[2], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(lout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Normal(sigma=.05), ctx=model_ctx, force_reinit=True)\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_arr = []\n",
    "for e in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        \n",
    "        with autograd.record():\n",
    "            data = mx.ndarray.cast(data, dtype='float32')\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "    \n",
    "    loss_arr.append(cumulative_loss/num_examples)\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "          (e, cumulative_loss/num_examples, train_accuracy, test_accuracy))    \n",
    "\n",
    "loss_batch_norm = loss_arr.copy()\n",
    "\n",
    "filename = os.path.join('weights','b_batch.params')\n",
    "net.save_parameters(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = [1024, 512, 256]\n",
    "lout = 10\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(layer[0], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dropout(.1))\n",
    "    net.add(gluon.nn.Dense(layer[1], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dropout(.1))\n",
    "    net.add(gluon.nn.Dense(layer[2], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dropout(.1))\n",
    "    net.add(gluon.nn.Dense(lout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Normal(sigma=.05), ctx=model_ctx, force_reinit=True)\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_arr = []\n",
    "for e in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        \n",
    "        with autograd.record():\n",
    "            data = mx.ndarray.cast(data, dtype='float32')\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "    \n",
    "    loss_arr.append(cumulative_loss/num_examples)\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "          (e, cumulative_loss/num_examples, train_accuracy, test_accuracy))    \n",
    "\n",
    "loss_dropout1 = loss_arr.copy()\n",
    "\n",
    "filename = os.path.join('weights','b_dropout1.params')\n",
    "net.save_parameters(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = [1024, 512, 256]\n",
    "lout = 10\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(layer[0], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dropout(.4))\n",
    "    net.add(gluon.nn.Dense(layer[1], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dropout(.4))\n",
    "    net.add(gluon.nn.Dense(layer[2], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dropout(.4))\n",
    "    net.add(gluon.nn.Dense(lout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Normal(sigma=.05), ctx=model_ctx, force_reinit=True)\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_arr = []\n",
    "for e in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        \n",
    "        with autograd.record():\n",
    "            data = mx.ndarray.cast(data, dtype='float32')\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "    \n",
    "    loss_arr.append(cumulative_loss/num_examples)\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "          (e, cumulative_loss/num_examples, train_accuracy, test_accuracy))    \n",
    "\n",
    "loss_dropout4 = loss_arr.copy()\n",
    "\n",
    "filename = os.path.join('weights','b_dropout4.params')\n",
    "net.save_parameters(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = [1024, 512, 256]\n",
    "lout = 10\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(layer[0], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dropout(.6))\n",
    "    net.add(gluon.nn.Dense(layer[1], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dropout(.6))\n",
    "    net.add(gluon.nn.Dense(layer[2], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dropout(.6))\n",
    "    net.add(gluon.nn.Dense(lout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Normal(sigma=.05), ctx=model_ctx, force_reinit=True)\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_arr = []\n",
    "for e in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        \n",
    "        with autograd.record():\n",
    "            data = mx.ndarray.cast(data, dtype='float32')\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "    \n",
    "    loss_arr.append(cumulative_loss/num_examples)\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "          (e, cumulative_loss/num_examples, train_accuracy, test_accuracy))    \n",
    "\n",
    "loss_dropout6 = loss_arr.copy()\n",
    "\n",
    "filename = os.path.join('weights','b_dropout6.params')\n",
    "net.save_parameters(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = [1024, 512, 256]\n",
    "lout = 10\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(layer[0], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(layer[1], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(layer[2], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(lout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Normal(sigma=.05), ctx=model_ctx, force_reinit=True)\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_arr = []\n",
    "lr = .001\n",
    "for e in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        \n",
    "        with autograd.record():\n",
    "            data = mx.ndarray.cast(data, dtype='float32')\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        w = []\n",
    "        for i in range(3):\n",
    "            w.append(net[i].weight.data())\n",
    "            w.append(net[i].bias.data())\n",
    "        sgd(w, lr, data.shape[0])\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "    \n",
    "    loss_arr.append(cumulative_loss/num_examples)\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "          (e, cumulative_loss/num_examples, train_accuracy, test_accuracy))    \n",
    "    \n",
    "loss_sgd = loss_arr.copy()\n",
    "\n",
    "filename = os.path.join('weights','b_sgd.params')\n",
    "net.save_parameters(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov’s accelerated momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Normal(sigma=.05), ctx=model_ctx, force_reinit=True)\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(),'nag', {'momentum':.1, 'learning_rate':.001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152843.2119860649\n",
      "Epoch 0. Loss: 232.63806999401052, Train_acc 0.8033809523809524, Test_acc 0.7902222222222223\n",
      "21554.34310722351\n",
      "Epoch 1. Loss: 32.80721934128388, Train_acc 0.8183095238095238, Test_acc 0.8040555555555555\n",
      "18685.60315513611\n",
      "Epoch 2. Loss: 28.440796278745978, Train_acc 0.7682380952380953, Test_acc 0.7493333333333333\n",
      "17098.82873606682\n",
      "Epoch 3. Loss: 26.02561451456137, Train_acc 0.8574285714285714, Test_acc 0.8347222222222223\n",
      "15851.296346902847\n",
      "Epoch 4. Loss: 24.126782871998245, Train_acc 0.8712619047619048, Test_acc 0.8421111111111111\n",
      "14877.305012226105\n",
      "Epoch 5. Loss: 22.644299866401987, Train_acc 0.8432142857142857, Test_acc 0.8123333333333334\n",
      "14071.034298181534\n",
      "Epoch 6. Loss: 21.41709938840416, Train_acc 0.8558571428571429, Test_acc 0.8219444444444445\n",
      "13417.7857131958\n",
      "Epoch 7. Loss: 20.422809304712025, Train_acc 0.8868809523809524, Test_acc 0.8516111111111111\n",
      "12805.385511398315\n",
      "Epoch 8. Loss: 19.49069332024097, Train_acc 0.898, Test_acc 0.8581666666666666\n",
      "12321.379415750504\n",
      "Epoch 9. Loss: 18.75400215487139, Train_acc 0.8926904761904761, Test_acc 0.8513888888888889\n"
     ]
    }
   ],
   "source": [
    "loss_arr = []\n",
    "for e in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        \n",
    "        with autograd.record():\n",
    "            data = mx.ndarray.cast(data, dtype='float32')\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        w = []\n",
    "        trainer.step(data.shape[0])\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "   \n",
    "    loss_arr.append(cumulative_loss/num_examples)\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(cumulative_loss)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "          (e, cumulative_loss/num_examples, train_accuracy, test_accuracy))    \n",
    "    \n",
    "loss_nest_opt = loss_arr.copy()\n",
    "\n",
    "filename = os.path.join('weights','b_nag.params')\n",
    "net.save_parameters(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaDelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Normal(sigma=.05), ctx=model_ctx, force_reinit=True)\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(),'adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_arr = []\n",
    "for e in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        \n",
    "        with autograd.record():\n",
    "            data = mx.ndarray.cast(data, dtype='float32')\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "    \n",
    "    loss_arr.append(cumulative_loss/num_examples)\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "          (e, cumulative_loss/num_examples, train_accuracy, test_accuracy))    \n",
    "\n",
    "loss_adadelta_opt = loss_arr.copy()\n",
    "\n",
    "filename = os.path.join('weights','b_adadelta.params')\n",
    "net.save_parameters(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Normal(sigma=.05), ctx=model_ctx, force_reinit=True)\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adagrad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_arr = []\n",
    "for e in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        \n",
    "        with autograd.record():\n",
    "            data = mx.ndarray.cast(data, dtype='float32')\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "    \n",
    "    loss_arr.append(cumulative_loss/num_examples)\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "          (e, cumulative_loss/num_examples, train_accuracy, test_accuracy))   \n",
    "    \n",
    "loss_adagrad_opt = loss_arr.copy()\n",
    "\n",
    "filename = os.path.join('weights','b_adagrad.params')\n",
    "net.save_parameters(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Normal(sigma=.05), ctx=model_ctx, force_reinit=True)\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_arr = []\n",
    "for e in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        \n",
    "        with autograd.record():\n",
    "            data = mx.ndarray.cast(data, dtype='float32')\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "    \n",
    "    loss_arr.append(cumulative_loss/num_examples)\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "          (e, cumulative_loss/num_examples, train_accuracy, test_accuracy))    \n",
    "\n",
    "loss_rmsprop_opt = loss_arr.copy()\n",
    "\n",
    "filename = os.path.join('weights','b_rmsprop.params')\n",
    "net.save_parameters(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Normal(sigma=.05), ctx=model_ctx, force_reinit=True)\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_arr = []\n",
    "for e in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        \n",
    "        with autograd.record():\n",
    "            data = mx.ndarray.cast(data, dtype='float32')\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "    \n",
    "    loss_arr.append(cumulative_loss/num_examples)\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "          (e, cumulative_loss/num_examples, train_accuracy, test_accuracy))    \n",
    "    \n",
    "loss_adam_opt = loss_arr.copy()\n",
    "\n",
    "filename = os.path.join('weights','b_adam.params')\n",
    "net.save_parameters(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15,15))\n",
    "plt.plot(loss_vanilla, linewidth=2, label='Vanilla')\n",
    "plt.plot(loss_norm_init, linewidth=2, label='Normal Initialization')\n",
    "plt.plot(loss_xavier_init, linewidth=2, label='Xavier Initialization')\n",
    "plt.plot(loss_ortho_init, linewidth=2, label='Orthogonal Initialization')\n",
    "plt.legend(fontsize=15)\n",
    "plt.title('Initialization Methods Comparison')\n",
    "plt.xlabel('Epochs', fontsize=15)\n",
    "plt.ylabel('Loss', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15,15))\n",
    "plt.plot(loss_vanilla, linewidth=2, label='Vanilla')\n",
    "plt.plot(loss_batch_norm, linewidth=2, label='Batch Normalization')\n",
    "plt.legend(fontsize=15)\n",
    "plt.title('Normalization Methods Comparison')\n",
    "plt.xlabel('Epochs', fontsize=15)\n",
    "plt.ylabel('Loss', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15,15))\n",
    "plt.plot(loss_vanilla, linewidth=2, label='Vanilla')\n",
    "plt.plot(loss_dropout1, linewidth=2, label='Dropout(0.1)')\n",
    "plt.plot(loss_dropout4, linewidth=2, label='Dropout(0.4)')\n",
    "plt.plot(loss_dropout6, linewidth=2, label='Dropout(0.6)')\n",
    "plt.legend(fontsize=15)\n",
    "plt.title('Dropout Comparison')\n",
    "plt.xlabel('Epochs', fontsize=15)\n",
    "plt.ylabel('Loss', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15,15))\n",
    "plt.plot(loss_vanilla, linewidth=2, label='Vanilla')\n",
    "plt.plot(loss_sgd, linewidth=2, label='SGD Optimization')\n",
    "plt.plot(loss_nest_opt, linewidth=2, label='Nesterov\\'s Optimization')\n",
    "plt.plot(loss_adadelta_opt, linewidth=2, label='AdaDelta Optimization')\n",
    "# plt.plot(loss_adagrad_opt, linewidth=2, label='AdaGrad Optimization')\n",
    "plt.plot(loss_rmsprop_opt, linewidth=2, label='RMSProp Optimization')\n",
    "plt.plot(loss_adam_opt, linewidth=2, label='Adam Optimization')\n",
    "plt.legend(fontsize=15)\n",
    "plt.title('Optimization Methods Comparison')\n",
    "plt.xlabel('Epochs', fontsize=15)\n",
    "plt.ylabel('Loss', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = [1024, 512, 256]\n",
    "lout = 10\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(layer[0], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(layer[1], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(layer[2], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(lout))\n",
    "test_data = mx.gluon.data.DataLoader(CustomDataset('test', 'validation'), batch_size, last_batch='keep', shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join('weights','b_vanilla.params')\n",
    "if not (os.path.isfile(filename)):\n",
    "    print('No data for Vanilla')\n",
    "else:\n",
    "    net.load_parameters(filename, ctx=ctx)\n",
    "    print('Accuracy(Vanilla) = ' + str(100 * evaluate_accuracy(test_data, net)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join('weights','b_normal.params')\n",
    "if not (os.path.isfile(filename)):\n",
    "    print('No data for Normal Initialization')\n",
    "else:\n",
    "    net.load_parameters(filename, ctx=ctx)\n",
    "    print('Accuracy(Normal Initialization) = ' + str(100 * evaluate_accuracy(test_data, net)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join('weights','b_xavier.params')\n",
    "if not (os.path.isfile(filename)):\n",
    "    print('No data for Xavier Initialization')\n",
    "else:\n",
    "    net.load_parameters(filename, ctx=ctx)\n",
    "    print('Accuracy(Xavier Initialization) = ' + str(100 * evaluate_accuracy(test_data, net)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join('weights','b_ortho.params')\n",
    "if not (os.path.isfile(filename)):\n",
    "    print('No data for Orthogonal Initialization')\n",
    "else:\n",
    "    net.load_parameters(filename, ctx=ctx)\n",
    "    print('Accuracy(Orthogonal Initialization) = ' + str(100 * evaluate_accuracy(test_data, net)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = [1024, 512, 256]\n",
    "lout = 10\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.BatchNorm())\n",
    "    net.add(gluon.nn.Dense(layer[0], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(layer[1], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(layer[2], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(lout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join('weights','b_batch.params')\n",
    "if not (os.path.isfile(filename)):\n",
    "    print('No data for Batch Normalization')\n",
    "else:\n",
    "    net.load_parameters(filename, ctx=ctx, allow_missing=True, ignore_extra=True)\n",
    "    print('Accuracy(Batch Normalization) = ' + str(100 * evaluate_accuracy(test_data, net)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = [1024, 512, 256]\n",
    "lout = 10\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(layer[0], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dropout(.1))\n",
    "    net.add(gluon.nn.Dense(layer[1], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dropout(.1))\n",
    "    net.add(gluon.nn.Dense(layer[2], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dropout(.1))\n",
    "    net.add(gluon.nn.Dense(lout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join('weights','b_dropout1.params')\n",
    "if not (os.path.isfile(filename)):\n",
    "    print('No data for Dropout(0.1)')\n",
    "else:\n",
    "    net.load_parameters(filename, ctx=ctx)\n",
    "    print('Accuracy(Dropout(0.1)) = ' + str(100 * evaluate_accuracy(test_data, net)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = [1024, 512, 256]\n",
    "lout = 10\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(layer[0], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dropout(.4))\n",
    "    net.add(gluon.nn.Dense(layer[1], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dropout(.4))\n",
    "    net.add(gluon.nn.Dense(layer[2], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dropout(.4))\n",
    "    net.add(gluon.nn.Dense(lout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join('weights','b_dropout4.params')\n",
    "if not (os.path.isfile(filename)):\n",
    "    print('No data for Dropout(0.4)')\n",
    "else:\n",
    "    net.load_parameters(filename, ctx=ctx, allow_missing=True)\n",
    "    print('Accuracy(Dropout(0.4)) = ' + str(100 * evaluate_accuracy(test_data, net)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = [1024, 512, 256]\n",
    "lout = 10\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(layer[0], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dropout(.6))\n",
    "    net.add(gluon.nn.Dense(layer[1], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dropout(.6))\n",
    "    net.add(gluon.nn.Dense(layer[2], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dropout(.6))\n",
    "    net.add(gluon.nn.Dense(lout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join('weights','b_dropout6.params')\n",
    "if not (os.path.isfile(filename)):\n",
    "    print('No data for Dropout(0.6)')\n",
    "else:\n",
    "    net.load_parameters(filename, ctx=ctx)\n",
    "    print('Accuracy(Dropout(0.6)) = ' + str(100 * evaluate_accuracy(test_data, net)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = [1024, 512, 256]\n",
    "lout = 10\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(layer[0], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(layer[1], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(layer[2], activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(lout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join('weights','b_sgd.params')\n",
    "if not (os.path.isfile(filename)):\n",
    "    print('No data for SGD')\n",
    "else:\n",
    "    net.load_parameters(filename, ctx=ctx)\n",
    "    print('Accuracy(SGD) = ' + str(100 * evaluate_accuracy(test_data, net)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy(Nesterov's Accelerated Momentum) = 85.13888888888889%\n"
     ]
    }
   ],
   "source": [
    "filename = os.path.join('weights','b_nag.params')\n",
    "if not (os.path.isfile(filename)):\n",
    "    print('No data for Nesterov\\'s Accelerated Momentum')\n",
    "else:\n",
    "    net.load_parameters(filename, ctx=ctx)\n",
    "    print('Accuracy(Nesterov\\'s Accelerated Momentum) = ' + str(100 * evaluate_accuracy(test_data, net)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join('weights','b_adadelta.params')\n",
    "if not (os.path.isfile(filename)):\n",
    "    print('No data for AdaDelta')\n",
    "else:\n",
    "    net.load_parameters(filename, ctx=ctx)\n",
    "    print('Accuracy(AdaDelta) = ' + str(100 * evaluate_accuracy(test_data, net)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join('weights','b_adagrad.params')\n",
    "if not (os.path.isfile(filename)):\n",
    "    print('No data for AdaGrad')\n",
    "else:\n",
    "    net.load_parameters(filename, ctx=ctx)\n",
    "    print('Accuracy(AdaGrad) = ' + str(100 * evaluate_accuracy(test_data, net)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join('weights','b_rmsprop.params')\n",
    "if not (os.path.isfile(filename)):\n",
    "    print('No data for RMSProp')\n",
    "else:\n",
    "    net.load_parameters(filename, ctx=ctx)\n",
    "    print('Accuracy(RMSProp) = ' + str(100 * evaluate_accuracy(test_data, net)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join('weights','b_adam.params')\n",
    "if not (os.path.isfile(filename)):\n",
    "    print('No data for Adam')\n",
    "else:\n",
    "    net.load_parameters(filename, ctx=ctx)\n",
    "    print('Accuracy(Adam) = ' + str(100 * evaluate_accuracy(test_data, net)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
